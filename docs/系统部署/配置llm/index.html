<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="配置LLM#LLM的使用是SuperSonic的重要一环。能显著增强对用户的问题的理解能力，是通过对话形式与用户交互的基石之一。在本项目中对语言模型能力的应用主要在 LLM 和 Embedding 两方面, 且支持Java和Python两种访问语言模型的方式。
首先，关于LLM的配置，我们专门在launchers/standalone/src/main/resources下创建了一个supersonic-env.sh文件，用于LLM的配置，通过在这里配置，Java和Python两种访问LLM的方式都能生效。
JavaLLMProxy#服务默认启D动方式就为Java访问语言模型的方式，语言模型相关配置可直接通过YAML文件来配置，目前在配置中支持配置访问open-ai的key和LLM模型的名称
PythonLLMProxy#Python访问LLM的方式需要通过sh assembly/bin/supersonic-daemon.sh start pyllm来启动Python服务, 该命令同时也会启动Java服务, 但通过Python服务来访问LLM。 Python服务默认使用的模型中，LLM选用闭源模型 gpt-3.5-turbo，Embedding模型选用开源模型 GanymedeNil/text2vec-large-chinese。用户可以根据自己实际需求进行配置更改。
配置方式
LLM模型的配置#LLM模型相关的配置，在 supersonic/headless/core/src/main/python/config/run_config.ini 进行配置。 LLM默认采用OpenAI的闭源模型 gpt-3.5-turbo，用户可以根据自己的实际需要选择LLM模型的提供方，例如Azure、文心一言等。通过[LLMProvider]下的LLM_PROVIDER_NAME 变量进行配置。需要注意的是，现阶段支持配置的模型提供方必须能够被langchain所支持，提供方的名字可以在langchain文档中查询。 LLM的相关变量在[LLMModel]下进行配置，例如openAI的模型，需要提供 MODEL_NAME、OPENAI_API_KEY、TEMPERATURE 等参数配置。不同的LLM提供方需要的配置各不相同，用户可以根据实际情况配置相关变量。 Embedding模型配置#Embedding模型默认采用开源模型 GanymedeNil/text2vec-large-chinese。用户可以根据实际需要配置适合的Embedding模型；通过[Text2Vec]下 HF_TEXT2VEC_MODEL_NAME 变量进行配置，为了使用方便采用托管在HuggingFace的源，初次启动时自动下载模型文件。 LLM与embedding配置 FAQ#可以用开源的LLM模型替代OpenAI的GPT模型吗？ 暂时不能。我们测试过大部分主流的开源LLM，在实际使用中，在本项目需要LLM提供的逻辑推理和代码生成场景上，开源模型还不能满足需求。 我们会持续跟进开源LLM的最新进展，在有满足要求的开源LLM后，在项目中集成私有化部署开源LLM的能力。 可以用国产的闭源模型替代OpenAI的GPT模型吗？ 据部分用户反馈，在他们的场景下文心一言、混元等国产闭源模型的效果与GPT3.5差距不大；整体而言GPT3.5及GPT4适用的场景会更广泛一些。用户可以在自己的场景下修改LLM的相应配置，试一试实际效果。 GPT4、GPT3.5、GPT3.5-16k 这几个模型用哪个比较好？ GPT3.5、GPT3.5-16k 均能基本满足要求，但会有输出结果不稳定的情况；GPT3.5的token长度限制为4k，在现有CoT策略下，容易出现超过长度限制的情况。 GPT4的输出更稳定，但费用成本远超GPT3.5，可以根据实际使用场景进行选择。 Embedding模型用其他的可以吗？ 可以。可以以该项目text2vec的榜单作为参考，然后在HuggingFace找到对应模型的model card，修改HF_TEXT2VEC_MODEL_NAME变量的取值。 启动时，首次下载Embedding模型需要会链接HuggingFace的源进行下载，如果网络不通怎么办？ 可以到HuggingFace的官网找到对应的model card，然后将模型下载到本地。在supersonic/chat/core/src/main/python/config/run_config.ini 中将HF_TEXT2VEC_MODEL_NAME变量配置为模型所在的绝对路径。 LLM在text2sql中的应用#text2sql的功能实现，高度依赖对LLM的应用。通过LLM生成SQL的过程中，利用小样本(few-shots-examples)通过思维链(chain-of-thoughts)的方式对LLM in-context-learning的能力进行引导，对于生成较为稳定且符合下游语法解析规则的SQL非常重要。用户可以根据自身需要，对样本池及样本的数量进行配置，使其更加符合自身业务特点。
text2sql运行中更新配置的脚本#如果在启动项目后，用户需要对text2sql功能的相关配置进行调试，可以在修改相关配置文件后，通过以下2种方式让配置在项目运行中让配置生效。 执行 supersonic-daemon."><meta name=theme-color media="(prefers-color-scheme: light)" content="#ffffff"><meta name=theme-color media="(prefers-color-scheme: dark)" content="#343a40"><meta name=color-scheme content="light dark"><meta property="og:url" content="http://localhost:1313/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEllm/"><meta property="og:site_name" content="SuperSonic"><meta property="og:title" content="配置LLM"><meta property="og:description" content="配置LLM#LLM的使用是SuperSonic的重要一环。能显著增强对用户的问题的理解能力，是通过对话形式与用户交互的基石之一。在本项目中对语言模型能力的应用主要在 LLM 和 Embedding 两方面, 且支持Java和Python两种访问语言模型的方式。
首先，关于LLM的配置，我们专门在launchers/standalone/src/main/resources下创建了一个supersonic-env.sh文件，用于LLM的配置，通过在这里配置，Java和Python两种访问LLM的方式都能生效。
JavaLLMProxy#服务默认启D动方式就为Java访问语言模型的方式，语言模型相关配置可直接通过YAML文件来配置，目前在配置中支持配置访问open-ai的key和LLM模型的名称
PythonLLMProxy#Python访问LLM的方式需要通过sh assembly/bin/supersonic-daemon.sh start pyllm来启动Python服务, 该命令同时也会启动Java服务, 但通过Python服务来访问LLM。 Python服务默认使用的模型中，LLM选用闭源模型 gpt-3.5-turbo，Embedding模型选用开源模型 GanymedeNil/text2vec-large-chinese。用户可以根据自己实际需求进行配置更改。
配置方式
LLM模型的配置#LLM模型相关的配置，在 supersonic/headless/core/src/main/python/config/run_config.ini 进行配置。 LLM默认采用OpenAI的闭源模型 gpt-3.5-turbo，用户可以根据自己的实际需要选择LLM模型的提供方，例如Azure、文心一言等。通过[LLMProvider]下的LLM_PROVIDER_NAME 变量进行配置。需要注意的是，现阶段支持配置的模型提供方必须能够被langchain所支持，提供方的名字可以在langchain文档中查询。 LLM的相关变量在[LLMModel]下进行配置，例如openAI的模型，需要提供 MODEL_NAME、OPENAI_API_KEY、TEMPERATURE 等参数配置。不同的LLM提供方需要的配置各不相同，用户可以根据实际情况配置相关变量。 Embedding模型配置#Embedding模型默认采用开源模型 GanymedeNil/text2vec-large-chinese。用户可以根据实际需要配置适合的Embedding模型；通过[Text2Vec]下 HF_TEXT2VEC_MODEL_NAME 变量进行配置，为了使用方便采用托管在HuggingFace的源，初次启动时自动下载模型文件。 LLM与embedding配置 FAQ#可以用开源的LLM模型替代OpenAI的GPT模型吗？ 暂时不能。我们测试过大部分主流的开源LLM，在实际使用中，在本项目需要LLM提供的逻辑推理和代码生成场景上，开源模型还不能满足需求。 我们会持续跟进开源LLM的最新进展，在有满足要求的开源LLM后，在项目中集成私有化部署开源LLM的能力。 可以用国产的闭源模型替代OpenAI的GPT模型吗？ 据部分用户反馈，在他们的场景下文心一言、混元等国产闭源模型的效果与GPT3.5差距不大；整体而言GPT3.5及GPT4适用的场景会更广泛一些。用户可以在自己的场景下修改LLM的相应配置，试一试实际效果。 GPT4、GPT3.5、GPT3.5-16k 这几个模型用哪个比较好？ GPT3.5、GPT3.5-16k 均能基本满足要求，但会有输出结果不稳定的情况；GPT3.5的token长度限制为4k，在现有CoT策略下，容易出现超过长度限制的情况。 GPT4的输出更稳定，但费用成本远超GPT3.5，可以根据实际使用场景进行选择。 Embedding模型用其他的可以吗？ 可以。可以以该项目text2vec的榜单作为参考，然后在HuggingFace找到对应模型的model card，修改HF_TEXT2VEC_MODEL_NAME变量的取值。 启动时，首次下载Embedding模型需要会链接HuggingFace的源进行下载，如果网络不通怎么办？ 可以到HuggingFace的官网找到对应的model card，然后将模型下载到本地。在supersonic/chat/core/src/main/python/config/run_config.ini 中将HF_TEXT2VEC_MODEL_NAME变量配置为模型所在的绝对路径。 LLM在text2sql中的应用#text2sql的功能实现，高度依赖对LLM的应用。通过LLM生成SQL的过程中，利用小样本(few-shots-examples)通过思维链(chain-of-thoughts)的方式对LLM in-context-learning的能力进行引导，对于生成较为稳定且符合下游语法解析规则的SQL非常重要。用户可以根据自身需要，对样本池及样本的数量进行配置，使其更加符合自身业务特点。
text2sql运行中更新配置的脚本#如果在启动项目后，用户需要对text2sql功能的相关配置进行调试，可以在修改相关配置文件后，通过以下2种方式让配置在项目运行中让配置生效。 执行 supersonic-daemon."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="docs"><title>配置LLM | SuperSonic</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png><link rel=canonical href=http://localhost:1313/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEllm/><link rel=stylesheet href=/book.min.309b7ed028807cdb68d8d61e26d609f48369c098dbf5e4d8c0dcf4cdf49feafc.css integrity="sha256-MJt+0CiAfNto2NYeJtYJ9INpwJjb9eTYwNz0zfSf6vw=" crossorigin=anonymous><script defer src=/fuse.min.js></script><script defer src=/en.search.min.0f16961ee0886ad20bfa2408c9fc172f849ac26d951bfb6e0d4f96030257550e.js integrity="sha256-DxaWHuCIatIL+iQIyfwXL4Sawm2VG/tuDU+WAwJXVQ4=" crossorigin=anonymous></script></head><body><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>SuperSonic</span></a></h2><div class="book-search hidden"><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><script>document.querySelector(".book-search").classList.remove("hidden")</script><ul><li><a href=/docs/%E5%BF%AB%E9%80%9F%E4%BD%93%E9%AA%8C/>快速体验</a></li><li><a href=/docs/%E9%A1%B9%E7%9B%AE%E6%9E%B6%E6%9E%84/>项目架构</a></li><li><span>系统部署</span><ul><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E7%BC%96%E8%AF%91%E6%9E%84%E5%BB%BA/>编译构建</a></li><li><a href=/docs/%E7%B3%BB%E7%BB%9F%E9%83%A8%E7%BD%B2/%E9%85%8D%E7%BD%AEllm/ class=active>配置LLM</a></li></ul></li><li><span>Chat BI</span><ul><li><a href=/docs/chat-bi/%E5%8A%A9%E7%90%86%E7%AE%A1%E7%90%86/>助理管理</a></li><li><a href=/docs/chat-bi/%E6%8F%92%E4%BB%B6%E7%AE%A1%E7%90%86/>插件管理</a></li></ul></li><li><span>Headless BI</span><ul><li><a href=/docs/headless-bi/%E6%9E%84%E5%BB%BA%E6%A8%A1%E5%9E%8B/>构建模型</a></li><li><a href=/docs/headless-bi/%E6%8C%87%E6%A0%87%E7%AE%A1%E7%90%86/>指标管理</a></li></ul></li><li><a href=/docs/faq/>FAQ</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>配置LLM</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#javallmproxy>JavaLLMProxy</a></li><li><a href=#pythonllmproxy>PythonLLMProxy</a><ul><li></li></ul></li><li><a href=#llm在text2sql中的应用><strong>LLM在text2sql中的应用</strong></a></li><li><a href=#text2sql运行中更新配置的脚本><strong>text2sql运行中更新配置的脚本</strong></a></li></ul></nav></aside></header><article class="markdown book-article"><h1 id=配置llm>配置LLM
<a class=anchor href=#%e9%85%8d%e7%bd%aellm>#</a></h1><p>LLM的使用是SuperSonic的重要一环。能显著增强对用户的问题的理解能力，是通过对话形式与用户交互的基石之一。在本项目中对语言模型能力的应用主要在 LLM 和 Embedding 两方面, 且支持Java和Python两种访问语言模型的方式。</p><p>首先，关于LLM的配置，我们专门在launchers/standalone/src/main/resources下创建了一个supersonic-env.sh文件，用于LLM的配置，通过在这里配置，Java和Python两种访问LLM的方式都能生效。</p><h2 id=javallmproxy>JavaLLMProxy
<a class=anchor href=#javallmproxy>#</a></h2><p>服务默认启D动方式就为Java访问语言模型的方式，语言模型相关配置可直接通过YAML文件来配置，目前在配置中支持配置访问open-ai的key和LLM模型的名称</p><h2 id=pythonllmproxy>PythonLLMProxy
<a class=anchor href=#pythonllmproxy>#</a></h2><p>Python访问LLM的方式需要通过<code>sh assembly/bin/supersonic-daemon.sh start pyllm</code>来启动Python服务, 该命令同时也会启动Java服务, 但通过Python服务来访问LLM。
Python服务默认使用的模型中，LLM选用闭源模型 gpt-3.5-turbo，Embedding模型选用开源模型 GanymedeNil/text2vec-large-chinese。用户可以根据自己实际需求进行配置更改。</p><p><strong>配置方式</strong></p><h4 id=llm模型的配置>LLM模型的配置
<a class=anchor href=#llm%e6%a8%a1%e5%9e%8b%e7%9a%84%e9%85%8d%e7%bd%ae>#</a></h4><ol><li>LLM模型相关的配置，在 supersonic/headless/core/src/main/python/config/run_config.ini 进行配置。</li><li>LLM默认采用OpenAI的闭源模型 gpt-3.5-turbo，用户可以根据自己的实际需要选择LLM模型的提供方，例如Azure、文心一言等。通过[LLMProvider]下的LLM_PROVIDER_NAME 变量进行配置。需要注意的是，现阶段支持配置的模型提供方必须能够被langchain所支持，提供方的名字可以在langchain文档中查询。</li><li>LLM的相关变量在[LLMModel]下进行配置，例如openAI的模型，需要提供 MODEL_NAME、OPENAI_API_KEY、TEMPERATURE 等参数配置。不同的LLM提供方需要的配置各不相同，用户可以根据实际情况配置相关变量。</li></ol><h4 id=embedding模型配置>Embedding模型配置
<a class=anchor href=#embedding%e6%a8%a1%e5%9e%8b%e9%85%8d%e7%bd%ae>#</a></h4><ol><li>Embedding模型默认采用开源模型 GanymedeNil/text2vec-large-chinese。用户可以根据实际需要配置适合的Embedding模型；通过[Text2Vec]下 HF_TEXT2VEC_MODEL_NAME 变量进行配置，为了使用方便采用托管在HuggingFace的源，初次启动时自动下载模型文件。</li></ol><h4 id=llm与embedding配置-faq>LLM与embedding配置 FAQ
<a class=anchor href=#llm%e4%b8%8eembedding%e9%85%8d%e7%bd%ae-faq>#</a></h4><ol><li>可以用开源的LLM模型替代OpenAI的GPT模型吗？<ul><li>暂时不能。我们测试过大部分主流的开源LLM，在实际使用中，在本项目需要LLM提供的逻辑推理和代码生成场景上，开源模型还不能满足需求。</li><li>我们会持续跟进开源LLM的最新进展，在有满足要求的开源LLM后，在项目中集成私有化部署开源LLM的能力。</li></ul></li><li>可以用国产的闭源模型替代OpenAI的GPT模型吗？<ul><li>据部分用户反馈，在他们的场景下文心一言、混元等国产闭源模型的效果与GPT3.5差距不大；整体而言GPT3.5及GPT4适用的场景会更广泛一些。用户可以在自己的场景下修改LLM的相应配置，试一试实际效果。</li></ul></li><li>GPT4、GPT3.5、GPT3.5-16k 这几个模型用哪个比较好？<ul><li>GPT3.5、GPT3.5-16k 均能基本满足要求，但会有输出结果不稳定的情况；GPT3.5的token长度限制为4k，在现有CoT策略下，容易出现超过长度限制的情况。</li><li>GPT4的输出更稳定，但费用成本远超GPT3.5，可以根据实际使用场景进行选择。</li></ul></li><li>Embedding模型用其他的可以吗？<ul><li>可以。可以以该项目<a href=https://github.com/shibing624/text2vec>text2vec</a>的榜单作为参考，然后在HuggingFace找到对应模型的model card，修改HF_TEXT2VEC_MODEL_NAME变量的取值。</li></ul></li><li>启动时，首次下载Embedding模型需要会链接HuggingFace的源进行下载，如果网络不通怎么办？<ul><li>可以到HuggingFace的官网找到对应的model card，然后将模型下载到本地。在supersonic/chat/core/src/main/python/config/run_config.ini 中将HF_TEXT2VEC_MODEL_NAME变量配置为模型所在的绝对路径。</li></ul></li></ol><h2 id=llm在text2sql中的应用><strong>LLM在text2sql中的应用</strong>
<a class=anchor href=#llm%e5%9c%a8text2sql%e4%b8%ad%e7%9a%84%e5%ba%94%e7%94%a8>#</a></h2><p>text2sql的功能实现，高度依赖对LLM的应用。通过LLM生成SQL的过程中，利用小样本(few-shots-examples)通过思维链(chain-of-thoughts)的方式对LLM in-context-learning的能力进行引导，对于生成较为稳定且符合下游语法解析规则的SQL非常重要。用户可以根据自身需要，对样本池及样本的数量进行配置，使其更加符合自身业务特点。</p><h2 id=text2sql运行中更新配置的脚本><strong>text2sql运行中更新配置的脚本</strong>
<a class=anchor href=#text2sql%e8%bf%90%e8%a1%8c%e4%b8%ad%e6%9b%b4%e6%96%b0%e9%85%8d%e7%bd%ae%e7%9a%84%e8%84%9a%e6%9c%ac>#</a></h2><ol><li>如果在启动项目后，用户需要对text2sql功能的相关配置进行调试，可以在修改相关配置文件后，通过以下2种方式让配置在项目运行中让配置生效。<ul><li>执行 supersonic-daemon.sh reload llmparser</li><li>执行 python examples_reload_run.py</li></ul></li></ol></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#javallmproxy>JavaLLMProxy</a></li><li><a href=#pythonllmproxy>PythonLLMProxy</a><ul><li></li></ul></li><li><a href=#llm在text2sql中的应用><strong>LLM在text2sql中的应用</strong></a></li><li><a href=#text2sql运行中更新配置的脚本><strong>text2sql运行中更新配置的脚本</strong></a></li></ul></nav></div></aside></main></body></html>